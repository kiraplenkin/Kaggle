#%%
import os
import re
import operator
import pandas as pd
import numpy as np
from tqdm import tqdm
from gensim.models import KeyedVectors

tqdm.pandas()

#%%
train = pd.read_csv('/Users/kirr/Pyth/Notebooks/Quora/data/train.csv')
test = pd.read_csv('/Users/kirr/Pyth/Notebooks/Quora/data/test.csv')

#%%
print('Train shape: ', train.shape)
print('Test shape: ', test.shape)

#%%
def build_vocab(sentences, verbose=True):
    # param sentences: list of list of words
    # return: dictionary of words and their count
    vocab = {}
    for sentence in tqdm(sentences, disable=(not verbose)):
        for word in sentence:
            try:
                vocab[word] += 1
            except KeyError:
                vocab[word] = 1
    return vocab

#%%
sentences = train['question_text'].progress_apply(lambda x: x.split()).values
vocab = build_vocab(sentences)
print({k: vocab[k] for k in list(vocab)[:5]})

#%%
news_path = '/Users/kirr/Pyth/Notebooks/Quora/data/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'
embedding_index = KeyedVectors.load_word2vec_format(news_path, binary=True)

#%%
def check_coverage(vocab, embedding_index):
    a = {}
    oov = {}
    k = 0
    i = 0
    for word in tqdm(vocab):
        try:
            a[word] = embedding_index[word]
            k += vocab[word]
        except:
            oov[word] = vocab[word]
            i += vocab[word]
            pass

    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))
    print('Found embeddings for {:.2%} of all text'.format(k / (k + i)))

    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]

    return sorted_x

#%%
oov = check_coverage(vocab, embedding_index)

#%%
oov[:10]

#%%
def clean_text(x):
    x = str(x)
    for punct in "/-'":
        x = x.replace(punct, ' ')
    for punct in '&':
        x = x.replace(punct, f' {punct} ')
    for punct in '?!.,"#$%\'()*+-/:;<=>@[\\]^_`{|}~' + '“”’':
        x = x.replace(punct, '')
    return x

#%%
train['question_text'] = train['question_text'].progress_apply(lambda x: clean_text(x))
sentences = train['question_text'].apply(lambda x: x.split())
vocab = build_vocab(sentences)

#%%
oov = check_coverage(vocab, embedding_index)

#%%
oov[:10]

#%%
for i in range(10):
    print(embedding_index.index2entity[i])

#%%
def clean_numbers(x):
    x = re.sub('[0-9]{5,}', '#####', x)
    x = re.sub('[0-9]{4,}', '####', x)
    x = re.sub('[0-9]{3,}', '###', x)
    x = re.sub('[0-9]{2,}', '##', x)
    return x

#%%
train['question_text'] = train['question_text'].progress_apply(lambda x: clean_numbers(x))
sentences = train['question_text'].progress_apply(lambda x: x.split())
vocab = build_vocab(sentences)

#%%
def _get_mispell(mispell_dict):
    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))
    return mispell_dict, mispell_re

mispell_dict = {'colour':'color',
                'centre':'center',
                'didnt':'did not',
                'doesnt':'does not',
                'isnt':'is not',
                'shouldnt':'should not',
                'favourite':'favorite',
                'travelling':'traveling',
                'counselling':'counseling',
                'theatre':'theater',
                'cancelled':'canceled',
                'labour':'labor',
                'organisation':'organization',
                'wwii':'world war 2',
                'citicise':'criticize',
                'instagram': 'social medium',
                'whatsapp': 'social medium',
                'snapchat': 'social medium'
                }

mispellings, mispellings_re = _get_mispell(mispell_dict)

def replace_typical_misspell(text): 
    def replace(match):
        return mispellings[match.group(0)]

    return mispellings_re.sub(replace, text)

#%%
train["question_text"] = train["question_text"].progress_apply(lambda x: replace_typical_misspell(x))
sentences = train["question_text"].progress_apply(lambda x: x.split())
to_remove = ['a','to','of','and']
sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]
vocab = build_vocab(sentences)

#%%
oov = check_coverage(vocab, embedding_index)